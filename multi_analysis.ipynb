{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711cebb2-09ee-4179-b607-af8ea87d293c",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4377aaa7-d391-497a-a5e5-8879176c6f8d",
   "metadata": {},
   "source": [
    "Washington State University's Decision Aid System as well as others have provided tree fruit growers with population phenology models for the common pest known as codling moth (_Cydia pomonella_) for years which have been shown to be quite reliable, and much research has been done into the population dynamics of codling moth subject to the manipulation various independent variables. One inconsistency relayed to the WSU CAHNRS Dept. of Entomology by growers is a discrepancy in the time our models expect the first emergences of the overwintering generation, and when their traps actually find evidence of them. Some organizations have asserted that these first trap captures are indicitive of the _real_ first emergence times, stating that they should be used as a sort of \"biofix\" that the models must be translated over to match. The fact that the same growers report significantly better matching in the second and third generations suggests to us that this is not the case, and we hypothesize that it is the result of sampling error that causes the difference in time between first emergence and first capture in these systems. Due to the very monolithic management of real orchards making field data not useful, and the very long turnaround times for new lab data to become available, it has been opted to use a simulation generating data matching key expectations gathered in previous lab studies to investigate the problem. The simulation is used to generate a swathe of data as a convolution of three varying degrees of freedom in the starting conditions (starting population size of the overwintering generation, percentage chance to capture a moth on a given degree day, and the average mating delay experienced by the moths)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c2551-2e47-45a2-a181-17c264139006",
   "metadata": {},
   "source": [
    "## Import libraries and split into individual dataframes for each permutation of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bac5dc-7151-4c3d-b5da-916cde10a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "df = pd.read_csv('test_1.csv')\n",
    "dfs = np.array_split(df, len(df)/df.dd_span.head(1).iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a92638-261e-4da2-81ca-76ac0dad1959",
   "metadata": {},
   "source": [
    "## Compile data about capture-emergence difference and clean instances with no first generation captures and/or first generation emergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0082649-d2c5-442f-823d-d070ab9a95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_diffs(dfs, thresh: int):\n",
    "    capture_data = []\n",
    "    for frame in dfs:\n",
    "        first_capture = frame[frame.captured > thresh].head(1).dd\n",
    "        first_emergence = frame[(frame.pop_active_0 > thresh)].head(1).dd\n",
    "        if not first_capture.empty and not first_emergence.empty:\n",
    "            diff = first_capture.iloc[0] - first_emergence.iloc[0]\n",
    "            pop_0 = frame.head(1).pop_0.iloc[0]\n",
    "            prob_capture = frame.head(1).prob_capture.iloc[0]\n",
    "            mating_delay = frame.head(1).mating_delay.iloc[0]\n",
    "            ratio_naive = pop_0 * prob_capture\n",
    "            ratio = (pop_0 ** 1/3) * (prob_capture ** 3)\n",
    "            capture_data.append((diff, ratio, ratio_naive, pop_0, prob_capture, mating_delay))\n",
    "    return capture_data\n",
    "\n",
    "first_capture_data = compile_diffs(dfs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd8b07f-fcea-4d9f-ac9c-b4219435b415",
   "metadata": {},
   "source": [
    "Two ratios are generated here for combinations between overwintering pop size and capture probability, a naive ratio just by multiplying them together (yes, I know this technically isn't a ratio since it's not divison, but since one of these values is very big and the other very small, multiplying gives much nicer numbers and is no less valid as a combination of the two than if I chose to multiply), and a better ratio by multiplying the cube root of the population to the cube of the capture probability. This balances out their relative importance in determining the difference much better, as will be demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febdbad-f3eb-4fb6-bbea-f2be810d69e8",
   "metadata": {},
   "source": [
    "## Organize data into three dataframes, a unified frame, an explanitory varaible frame, and a dependent variable frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc0222-ee75-4a82-bc8c-7c002e54727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_df = pd.DataFrame(first_capture_data, columns=['difference', 'ratio', 'naive_ratio', 'pop_0', 'prob_capture', 'mating_delay'])\n",
    "df_x = capture_df.drop('difference', axis=1)\n",
    "df_y = capture_df['difference']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d459f09-6d7c-49b3-8ec9-2e4c91e6393d",
   "metadata": {},
   "source": [
    "## View variables against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd1831-1556-492e-8bf2-951a078035df",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_df.plot.scatter(1, 0, 1.5, logy=False)\n",
    "capture_df.plot.scatter(2, 0, 1.5, logy=False)\n",
    "capture_df.plot.scatter(3, 0, 1.5, logy=False)\n",
    "capture_df.plot.scatter(4, 0, 1.5, logy=False)\n",
    "capture_df.plot.scatter(5, 0, 1.5, logy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed5b356-0813-4c35-96cf-e94dfeb0275b",
   "metadata": {},
   "source": [
    "## Create function for automatically fitting reciprocal functions using inverse-composition linearization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25940ab2-d33a-47da-9b31-142785da6d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autofit_reciprocal(x, y, a_guess, b_guess, c_guess, full_output=False):\n",
    "    a = a_guess\n",
    "    b = b_guess\n",
    "    c = c_guess\n",
    "    slope = 2\n",
    "    intercept = 1\n",
    "\n",
    "    # fit 'b' parameter\n",
    "    magnitude = 1\n",
    "    for _ in range(1,50):\n",
    "        y_linear = np.power(np.multiply(np.reciprocal(np.subtract(y, b)), a), 1 / c)\n",
    "        p = np.polyfit(x, y_linear, 1, w=np.log(y))\n",
    "        b_below = b + (0.1 ** magnitude)\n",
    "        b_above = b - (0.1 ** magnitude)\n",
    "        y_linear_above = np.power(np.multiply(np.reciprocal(np.subtract(y, b_above)), a), 1 / c)\n",
    "        y_linear_below = np.power(np.multiply(np.reciprocal(np.subtract(y, b_below)), a), 1 / c)\n",
    "        p_above = np.polyfit(x, y_linear_above, 1, w=np.log(y))\n",
    "        p_below = np.polyfit(x, y_linear_below, 1, w=np.log(y))\n",
    "        if abs(p[1]) < abs(p_above[1]) and abs(p[1]) < abs(p_below[1]):\n",
    "            magnitude = magnitude + 1\n",
    "        elif abs(p_above[1]) < abs(p_below[1]):\n",
    "            b = b_above\n",
    "        elif abs(p_below[1]) < abs(p_above[1]):\n",
    "            b = b_below\n",
    "\n",
    "    # fit 'a' parameter\n",
    "    magnitude = 0\n",
    "    for _ in range(1,50):\n",
    "        y_linear = np.power(np.multiply(np.reciprocal(np.subtract(y, b)), a), 1 / c)\n",
    "        p = np.polyfit(x, y_linear, 1, w=np.log(y))\n",
    "        a_below = a + (0.1 ** magnitude)\n",
    "        a_above = a - (0.1 ** magnitude)\n",
    "        y_linear_above = np.power(np.multiply(np.reciprocal(np.subtract(y, b)), a_above), 1 / c)\n",
    "        y_linear_below = np.power(np.multiply(np.reciprocal(np.subtract(y, b)), a_below), 1 / c)\n",
    "        p_above = np.polyfit(x, y_linear_above, 1, w=np.log(y))\n",
    "        p_below = np.polyfit(x, y_linear_below, 1, w=np.log(y))\n",
    "        # print(f'{a}, {p_above[0]}, {p[0]}, {p_below[0]}')\n",
    "        if abs(p[0] - 1) < abs(p_above[0] - 1) and abs(p[0] - 1) < abs(p_below[0] - 1):\n",
    "            magnitude = magnitude + 1\n",
    "        elif abs(p_above[0] - 1) < abs(p_below[0] - 1):\n",
    "            a = a_above\n",
    "        elif abs(p_below[0] - 1) < abs(p_above[0] - 1):\n",
    "            a = a_below\n",
    "    if full_output:\n",
    "        return (a, b, p)\n",
    "    else:\n",
    "        return (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fdf7e9-d65d-4149-8c0f-583dfec1168e",
   "metadata": {},
   "source": [
    "## Fit curve to data using naive ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9807e74-d1fd-4323-a974-e5694fddf033",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = capture_df.naive_ratio\n",
    "y = capture_df.difference\n",
    "\n",
    "c = 0.29\n",
    "a, b, p = autofit_reciprocal(x, y, 3, 0.3, c, full_output=True)\n",
    "print(*p)\n",
    "\n",
    "y_linear_naive = np.power(np.multiply(np.reciprocal(np.subtract(y, b)), a), 1 / c)\n",
    "\n",
    "fitted_x = np.linspace(x.min(), x.max(),1000)\n",
    "fitted_y_naive = np.add(np.multiply(np.reciprocal(np.power(x, c)), a), b)\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.scatter(x, y, label='Data', s=2)\n",
    "ax.scatter(x, fitted_y_naive, label='Prediction', s=1)\n",
    "ax.set_title('Fitted curve for ratio to difference relationship')\n",
    "ax.set_ylabel('ln( difference )')\n",
    "ax.set_yscale('log') # view in log to emphasize small differences\n",
    "ax.set_ylim(1, 80)\n",
    "ax.set_xlabel('ratio')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3b107-6153-4a72-aae7-1afa1c66831d",
   "metadata": {},
   "source": [
    "## Check residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae87269-bafc-42ca-bcdf-de61f4b3e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes()\n",
    "ax.scatter(x, np.subtract(y, fitted_y_naive), label='Data', s=2)\n",
    "ax.set_title('Residuals vs. ratio')\n",
    "ax.set_ylabel('Actual value - predicted value')\n",
    "ax.set_xlabel('ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60af811-0cd6-4652-a42a-ab6d96abb8b3",
   "metadata": {},
   "source": [
    "+/- 10 degree day heteroscedasticity could definitely be improved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ced304-b3da-4dc9-b480-61042843329a",
   "metadata": {},
   "source": [
    "## Fit curve to data using smarter ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7058e4-e596-420d-b518-fd500f556bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = capture_df.ratio\n",
    "y = capture_df.difference\n",
    "\n",
    "c = 0.2\n",
    "a, b = autofit_reciprocal(x, y, 3, 0.7, c)\n",
    "\n",
    "y_linear = np.power(np.multiply(np.reciprocal(np.subtract(y, b)), a), 1 / c)\n",
    "\n",
    "fitted_x = np.linspace(x.min(), x.max(),1000)\n",
    "fitted_y = np.add(np.multiply(np.reciprocal(np.power(x, c)), a), b)\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.scatter(x, y, label='Data', s=2)\n",
    "ax.scatter(x, fitted_y, label='Prediction', s=1)\n",
    "ax.set_title('Fitted curve for ratio to difference relationship')\n",
    "ax.set_ylabel('ln( difference )')\n",
    "ax.set_yscale('log') # view in log to emphasize small differences\n",
    "ax.set_ylim(1, 80)\n",
    "ax.set_xlabel('ratio')\n",
    "ax.legend()\n",
    "print((a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56bdd41-d74f-4d94-b195-c2b57826baa0",
   "metadata": {},
   "source": [
    "For the ratio function of (pop_0^1/3) * (prob_capture^3), a function of y = (3.024 / x ^ 0.2) + 0.965 predicts the difference in first emergence and first capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ab174-99c4-4f07-9e75-db18f42335d0",
   "metadata": {},
   "source": [
    "## Determine goodness of fit by looking at residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e2831-5ebb-426c-be50-c58f3397d253",
   "metadata": {},
   "source": [
    "### Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1d2fd-0347-4515-b33c-733063f79e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes()\n",
    "ax.scatter(x, np.subtract(y, fitted_y), label='Data', s=2)\n",
    "ax.set_title('Residuals vs. ratio')\n",
    "ax.set_ylabel('Actual value - predicted value')\n",
    "ax.set_xlabel('ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4ddda-3350-4606-8a7f-6f08366bf4c0",
   "metadata": {},
   "source": [
    "Definitely still some heteroscedasticity, but it looks like the worst of it is only +/- 3, and the majority of points seem to be in the +/- 1 area. Measureable improvement from the naive ratio. Furthermore, from looking at the fitted curve graphs of the two, no value of 'c' was able to match both the extreme and median points to a satisfactory degree with the naive ratio, but a 'c' of 0.2 matches both quite well with the smarter combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771c019-f3d9-48c4-b2a9-3c701d1ee82a",
   "metadata": {},
   "source": [
    "### Measure how bad the heteroscedasticity is within the unstable small values area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753623c-e655-4088-b0ee-a780c2567981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "rolling_avg = gaussian_filter1d(np.subtract(y, fitted_y), 15, mode='nearest')\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.scatter(x, np.subtract(y, fitted_y), label='Data', s=2)\n",
    "ax.scatter(x, rolling_avg, label='Gaussian Rolling Avg', s=1)\n",
    "ax.set_title('Residuals vs. ratio')\n",
    "ax.set_ylabel('Actual value - predicted value')\n",
    "ax.set_xlabel('ratio')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3cbc29-cb94-4ac0-93a7-ca195cf08397",
   "metadata": {},
   "source": [
    "With a gaussian weighted rolling average with a standard deviation of 15, it does appear the vast majority of all residuals hang within the +/- 1.0 range, which is definitely below noise for field emergences/captures. I am quite satisfied with this, though I do think the ultimate root cause of the heteroscedasticity could probably be weeded out given enough time and the residuals could be made 100% normally distributed. For now, this is well within the bounds of utility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d8497-af50-4839-9f9d-e474dd334d91",
   "metadata": {},
   "source": [
    "### Average bias per point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502128e-dbc9-4183-8002-55e94ebe1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.subtract(y, fitted_y)) / len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e367807a-41a8-49b2-af89-9b74e9cf6386",
   "metadata": {},
   "source": [
    "The fitted curve is a little biased negative relative to the actual data, but I again feel that less than a fifth of a degree day average negative bias is satisfactory for practical use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f37290b-e903-4145-9ee9-55da5267fd47",
   "metadata": {},
   "source": [
    "## Investigate how the difference evolves when looking at second captures, third captures, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6621cb-0826-45cc-b31d-25274b5fc5f9",
   "metadata": {},
   "source": [
    "### Compile 1st through 30th differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4003608-218c-47df-88c7-14909dcd0dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for i in range(1,31):\n",
    "    row = compile_diffs(dfs, i)\n",
    "    row_df = pd.DataFrame(row, columns=['difference', 'ratio', 'naive_ratio', 'pop_0', 'prob_capture', 'mating_delay'])\n",
    "    diffs.append(row_df)\n",
    "\n",
    "len(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04eb522-8597-495b-b4c3-b8cd2d2703eb",
   "metadata": {},
   "source": [
    "### Look at the graphs of the second, third, tenth, and thirtieth differences against the ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41300f-c590-48c6-93ab-2f6a3ef5d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs[1].plot.scatter(1, 0, 1.5, title='Second diffs', logy=True)\n",
    "diffs[2].plot.scatter(1, 0, 1.5, title='Third diffs', logy=True)\n",
    "diffs[4].plot.scatter(1, 0, 1.5, title='FIfth diffs', logy=True)\n",
    "diffs[9].plot.scatter(1, 0, 1.5, title='Tenth diffs', logy=True)\n",
    "diffs[19].plot.scatter(1, 0, 1.5, title='Twentieth diffs', logy=True)\n",
    "diffs[29].plot.scatter(1, 0, 1.5, title='Thirtieth diffs', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69f6040-3252-490e-bbcc-77c74c7a1d2a",
   "metadata": {},
   "source": [
    "It looks like the variance for small ratios gets greater as the difference being inspected increases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
